% https://math.la.asu.edu/~jtaylor/teaching/Fall2010/STP421/lectures/lecture20.pdf
\titledquestion{Sums of independent random variables}
Recall that if $f,g:\mathbb{R}\to\mathbb{R}$ are two integrable real-valued functions, then their convolution is the real-valued function $f*g:\mathbb{R}\to\mathbb{R}$ defined as 
\begin{flalign*}
    (f*g)(z)&=\int_{-\infty}^{+\infty}f(x)g(z-x)\,dx
\end{flalign*}
\begin{parts}
    \part[3] Show that $(f*g)(z)=(g*f)(z)$.
    \part[4] Show that $(f*(g*h))(z)=((f*g)*h)(z)$.
    \part[5] Let $X,Y$ be i.i.d. $\sim\mathcal{U}(0,1)$. Show that the density of $X+Y$ is given by 
    $$p_{X+Y}=\begin{cases}
        z&\text{ if $0\leq z\leq 1$}\\
        2-z&\text{ if $1<z\leq 2$}\\
        0&\text{ otherwise}.
    \end{cases}$$
    \part[5] Suppose now we have a sequence $X_1,\ldots,X_n$ of $n$ zero-mean Gaussian random variables where $X_i$ has variance $\sigma_i^2$ for every $i\in[n]$ and let $Z=X_1+\ldots+X_n$ be their sum. Show that $Z\sim\mathcal{N}(0,\sigma^2)$ where $\sigma^2=\sum_{i\in[n]}\sigma_i^2$.
\end{parts}
\newpage (this page is intentionally left blank)
\newpage

% /Users/seboll/Desktop/IN_MY_MIND/MATHS/ALGORITHMS_AND_TCS/random_matrices.pdf
\titledquestion{Eigenvalues of random matrices}
In this problem, we will discuss the computation of eigenvalues of random matrices. Let us start by considering a 2$\times$2 matrix $$H_s=\begin{pmatrix}X_1&X_3\\X_3&X_2\end{pmatrix},$$ where $X_1,X_2\sim\mathcal{N}(0,1)$ and $X_3\sim\mathcal{N}(0,1/2)$. We are interested in finding the density function $p(s)$ of the spacing $s=\lambda_2-\lambda_1$ between its two eigenvalues (where $\lambda_2>\lambda_1)$.
\begin{parts}
    \part[5] Recall that we can find a matrix' eigenvalues by computing the roots of its characteristic polynomial. Knowing this, show that $$\lambda_{1,2}=\frac{1}{2}\left(X_1+X_2\pm\sqrt{(X_1-X_2)^2+4X_3}\right).$$
    \part[3] Deduce that $$s=\sqrt{(X_1-X_2)^2+4X_3}.$$
    \part[10] Let $x_1,x_2,x_3$ be the values $X_1,X_2,X_3$ take, respectively. By definition,  we have $$p(s)=\int_{-\infty}^{+\infty}\frac{e^{-\frac{1}{2}x_1^2}}{\sqrt{2\pi}}\frac{e^{-\frac{1}{2}x_2^2}}{\sqrt{2\pi}}\frac{e^{-x_3^2}}{\sqrt{\pi}}\left(s-\sqrt{(x_1-x_2)^2+4x_3}\right)\,dx_1dx_2dx_3.$$
    We know as a fact that $p(s)$ is a valid pdf. Show that $$p(s)=\frac{s}{2}e^{-s^2/4}.$$ Hint: use a variant of the spherical coordinates by letting $x_1-x_2=r\cos\theta$, $2x_3=r\sin\theta$ and $x_1+x_2=\phi$.
    \part[10] Define $\overline{p}(s)=\langle s\rangle p\big(\langle s\rangle s\big)$, where $\langle s\rangle=\int_0^\infty sp(s)\,ds$ is the mean level spacing. Upon this rescaling, note that $$\int_0^\infty\overline{p}(s)\,ds=\int_0^\infty s\overline{p}(s)\,ds=1.$$ For the matrix $H_s$ defined earlier, show that $$\overline{p}(s)=\frac{\pi s}{2}e^{-\pi s^2/4}.$$ This function is known as Wigner's surmise and is mainly used to describe the density of level spacings of normalised eigenvalues.
\end{parts}
\newpage (this page is intentionally left blank)
\newpage (this page is intentionally left blank)
\newpage

\titledquestion{Locality Sensitive Hashing using cosine similarity}
In this exercise, we investigate the core theory behind the implementation of fast cosine similarity. Formally, we define the cosine similarity between two vectors as follows: let $u$ and $v$ be two vectors in a $k$-dimensional hyperplane. Let $\theta(u,v)$ denote the angle between the two vectors (measured in radians), $\langle u,v\rangle=\sum_i u_iv_i$ their dot product and $||u||_2,||v||_2$ their respective $L2$-norms. The cosine similarity is then given by $$\cossim(u,v)=\cos\theta(u,v)=\frac{\langle u,v\rangle}{||u||_2||v||_2}.$$
\begin{parts}
    % https://arxiv.org/pdf/2107.04071.pdf
    \part[4] Show that $\cossim(u,v)=\cossim(\alpha u,v)=\cossim(u,\alpha v)$ for any $\alpha>0$.
    \part[4] Recall that the Euclidean distance between two vectors $u$ and $v$ is given as $$d_E(u,v)=\sqrt{\sum_i (u_i-v_i)^2}.$$ Express the Euclidean distance in terms of the cosine similarity.
    \part[2] Deduce from the previous question that $$\cossim(u,v)=1-\frac{1}{2}d_E^2\big(u/||u||,v/||v||\big).$$
    \part[6] Suppose we are now given a collection of vectors in a $k$-dimensional vector space $\mathbb{R}^k$. Choose a family of hash functions as follows: generate a spherically symmetric random vector $r$ of unit length from this $k$-dimensional space and define a hash function $h_r$ as $$h_r(u)=\begin{cases}
        1&\text{ if $r.u\geq 0$}\\0&\text{ otherwise}.
    \end{cases}$$
    % https://aclanthology.org/P05-1077.pdf
    Prove that for two vectors $u$ and $v$, $${\bf Prob}(h_r(u)=h_r(v))=1-\frac{\theta(u,v)}{\pi}.$$
    \part[2] Deduce that $$\cossim(u,v)=\cos(1-{\bf Prob}(h_r(u)=h_r(v))\pi).$$
\end{parts}
\newpage (this page is intentionally left blank)
\newpage

% https://cs.brown.edu/courses/csci1550/homework/2021/hw0_2021_sol.pdf
% Problem 1
\titledquestion{Probabilistic analysis}
In this problem, we will use an unbiased coin to simulate a biased coin. We are given a random source $B$ that generates a sequence of independent bits $b_1,b_2,\ldots$ such that for each $i$, $Pr(b_i=0)=Pr(b_i=1)=1/2$ (i.e. a sequence of independent unbiased coin flips). Let $X$ denote the outcome of an biased coin flip, i.e. $X$ is a Bernoulli r.v. of parameter $p$ such that $Pr(X=1)=p$ and $Pr(X=0)=1-p$ for some $p\in[0,1]$. We want to generate a random value with the distribution of $X$ using the random source $B$. Furthermore, let $Y_j=\sum_{i=1}^j b_i2^{-j}$. Clearly for each $j$, the r.v. $Y_j$ is uniformly distributed in $\{0,2^{-j},2\cdot 2^{-j},\ldots,1\}$.\par 
Consider now the following procedure: We generate random values $y_1,y_2,\ldots$ by choosing random bits $b_1,b_2,\ldots$. We stop at the first $k$ such that either any extension of $y_k$ with more random bits is guaranteed to be $\leq p$ - in that case we output $X=1$ - or at the first $k$ such that any extension of $y_k$ is $>p$ - where in that case we output $X=0$.
\begin{parts}
    \part[5] Show that the above procedure generates a random variable $X$ with the correct distribution.
    \part[3] What is the probability that this procedure generates more than $l$ random bits ?
    \part[6] What is the expected number of bits generated by this procedure and what is the worst case number of bits possible ?
\end{parts}
\newpage (this page is intentionally left blank)
\newpage

% /Users/seboll/Desktop/IN_MY_MIND/MATHS/METHODES_MATHS/PROBABILITY/MIT18_S997S15_Chapter1.pdf
% 1.3 Sub-exponential
\titledquestion{Sub-exponential random variables}
In high dimensional probability, we say that a random variable $X$ follows a sub-exponential distribution with parameter $\lambda$ (that we write as $X\sim\subE(\lambda)$) if $E[X]=0$ and its moment generating function satisfies $E[e^{sX}]\leq e^{s^2\lambda^2/2}$, for all $|s|\leq 1/\lambda$. In this problem, we will see how to define such random variables and prove a useful lemma that involves their usage.
\begin{parts}
    \part[5] Let $X$ be a centered random variable such that ${\bf Prob}(|X|>t)\leq 2e^{-2t/\lambda}$ for some $\lambda>0$. Show that for any positive integer $k\geq 1$, $$E[|X|^k]\leq\lambda^k k!.$$
    \part[3] Deduce that $$\big(E[|X|^k]\big)^{1/k}\leq 2\lambda k.$$
    \part[5] Conclude that $X$ is sub-exponentially distributed, i.e. that its MGF satisfies $$E[e^{sX}]\leq e^{2s^2\lambda^2},$$ for all $|s|\leq 1/2\lambda$. Hint: use a Taylor expansion.\par
    We now recall the definition of a sub-Gaussian random variable. We say that a random variable $Y\in\mathbb{R}$ follows a sub-Gaussian distribution with variance proxy $\sigma^2$ if $E[Y]=0$ and its moment generating function satisfies $$E[e^{sY}]\leq e^{\sigma^2 s^2/2},\,\forall s\in\mathbb{R}.$$ Notice that its definition is close to that of the sub-exponential one.
    \part[10] Let $Y\sim\subG(\sigma^2)$. Show that the random variable $Z=Y^2-E[Y^2]$ is sub-exponential with parameter $16\sigma^2$. Hint: to prove this, you will have to use Jensen's inequality along with the following lemma:
    \begin{lemma}
        Let $X$ be a random variable such that ${\bf Prob}(|X|>t)\leq 2e^{-t^2/2\sigma^2}$. Then, for any positive integer $k\geq 1$, $E[|X|^k]\leq (2\sigma^2)^{k/2}k\Gamma(k/2)$ where $\Gamma(t)=\int_0^\infty x^{t-1}e^{-x}\,dx$ is the Gamma function defined for $t>0$.
    \end{lemma}
\end{parts}
\newpage (this page is intentionally left blank)
\newpage (this page is intentionally left blank)
\newpage

